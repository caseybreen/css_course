---
title: "Topics in Computational Social Science - Lab 3"
author: "Casey Breen"
date: "`r format(Sys.Date(), '%d %B %Y')`"
format: html
editor: visual
---

## Lab 3: Non-probability sampling

In this lab, we will explore different statistical methods for adjusting non-probability samples to improve their representativeness of the population of interest.

Non-probability sampling is increasingly common approach to data collection. While probability sampling is generally recommended whenever possible, logistical or financial challenges often preclude researchers from collecting a probability sample. For an overview of non-probability sampling in the social sciences, see:

-   Baker, Reg, J. Michael Brick, Nancy A. Bates, Mike Battaglia, Mick P. Couper, Jill A. Dever, Krista J. Gile, and Roger Tourangeau. 2013. ‘Summary Report of the AAPOR Task Force on Non-Probability Sampling’. Journal of Survey Statistics and Methodology 1(2):90–143. <https://doi.org/10.1093/jssam/smt008>.

-   Kennedy, Andrew Mercer, Arnold Lau and Courtney. 2018. ‘For Weighting Online Opt-In Samples, What Matters Most?’ Pew Research Center. Retrieved 10 February 2025 (<https://www.pewresearch.org/methods/2018/01/26/for-weighting-online-opt-in-samples-what-matters-most/>).

-   Lehdonvirta, Vili, Atte Oksanen, Pekka Räsänen, and Grant Blank. 2021. ‘Social Media, Web, and Panel Surveys: Using Non-Probability Samples in Social and Policy Research’. Policy & Internet 13(1):134–55. <https://www.doi.org/10.1002/poi3.238>

In the first exercise, we will use simulated data and construct poststratification weights. This will help us understand the role of weighting adjustments in improving the representativeness of non-probability samples. In the second exercise, we will work with real data from the Democratic Republic of the Congo to estimate crude death rates using a non-probability sample.

## Exercise 1

In this exercise, we'll construct *survey weights* for a simulated convenience sample. A survey weight is a numeric value assigned to each respondent in a survey to adjust for unequal probabilities of selection.

The big-picture goal of constructing survey weights is to adjust the sample so that it more accurately represents the population of interest. This is particularly important for non-probability samples, where certain groups are over- or underrepresented due to various selection biases.

There are many different methods for generating survey weights, and each method comes with a set of tradeoffs. The 'right' weighting strategy depends on the availability of auxiliary data, the sampling design, and the specific biases the researcher aims to correct. Ideally, the chosen weighting strategy will improve representativeness by aligning the sample with known population characteristics, while minimizing variance and other biases in estimating population-level parameters.

This (simulated) convenience sample was conducted at a coffee shop. The goal of survey was to estimate the average daily coffee consumption in cups among adults in Mochatown. Respondents age 18+ who consented to participate in the study were asked about their age, income, and daily coffee consumption.

To construct the poststratification weights, we have also obtained auxiliary city-level data on the proportion of people aged 18+ in each age X income group ("strata") in Melbourne. We will use this auxiliary data to help construct the poststratification weights.

```{r, output = F}
## library packages 
library(tidyverse)
library(cowplot)
```

First, we'll load in our data:

```{r}
## read in sample data 
coffee_shop_sample <- data.frame(
  id = 1:20,
  age_group = c("50+","50+","30-49",
                "50+","18-29","18-29","18-29","30-49","18-29","18-29",
                "18-29","18-29","50+","30-49","18-29","50+",
                "50+","30-49","18-29","18-29"),
  income_group = c("High","Middle","High",
                   "High","Middle","Low","Middle","High","Middle",
                   "High","Low","High","Middle","Low","Middle","High",
                   "Middle","Middle","High","Low"),
  coffee_consumption = c(2.7,1.2,2.8,4.2,4.9,
                         2.6,2.7,1.2,3.5,2.4,3.5,3.7,4,2.4,3.5,1.3,2.2,
                         2.1,0.6,3))
```

We'll also load our auxiliary data from the city on the true proportion of people in each group.

```{r}
## read in auxiliary population proportions 
pop_proportion <- data.frame(
  pop_proportion = c(0.12, 0.2, 0.08, 0.1, 0.25, 0.05, 0.05, 0.1, 0.05),
  age_group = c("18-29",
                "30-49","50+","18-29","30-49","50+","18-29",
                "30-49","50+"),
  income_group = c("Low","Low",
                   "Low","Middle","Middle","Middle","High",
                   "High","High"))
```

## Constructing poststratification weights

Poststratification weighting is a statistical re-weighting approach used to correct for differences between a sample and the population of interest.

To construct poststratification weights, the sample is split into mutually exclusive groups ("cells") based on characteristics (e.g., age, gender, and income). The weight for each person in a given cell $i$ is then calculated as the population proportion in cell $i$ divided by the sample proportion in cell $i$. The population proportion this ensures that underrepresented groups receive higher weights while overrepresented groups receive lower weights.

$$
\text{Weight}_{\text{cell}} = \frac{\text{Population Proportion}_{\text{cell}}}{\text{Sample Proportion}_{\text{cell}}}
$$

The code below defines cells based on income and age. It then generates poststratification weights.

```{r}
## calculate sample proportion 
sample_proportion <- coffee_shop_sample %>% 
  count(income_group, age_group) %>% ## define cells by income and age group 
  mutate(sample_proportion = n / sum(n))

## join population proportions together 
poststrat_weights <- sample_proportion %>% 
  inner_join(pop_proportion, by = join_by(age_group, income_group)) %>% 
  mutate(weight =  pop_proportion / sample_proportion) %>%      ## calculate weight 
  select(age_group, income_group, weight)

```

Now, we will assign these poststratification weights to the respondents in our sample. We will also re-scale the weights by dividing each weight by the average weight. This rescaling step ensures that the average weight is 1, which is helpful when calculating weighted sums and averages.

```{r}
## add on weights 
sample_data_weighted <- coffee_shop_sample %>% 
  left_join(poststrat_weights, by = c("age_group", "income_group"))

## re-scale weights so average weight is 1 
sample_data_weighted <- sample_data_weighted %>% 
  mutate(weight = weight/mean(weight)) 
```

## Exercise 1 questions

1.  Make a histogram of the distribution of the weights using `ggplot()` and `geom_histogram()`
2.  Calculate the mean (average) weight. Did our rescaling work?
3.  Estimate the weighted and unweighted number of cups of coffee consumed daily. Which estimate is larger?
4.  Do we trust our new weighted estimates of the average number of coffee drinkers? In what ways might our re-weighted sample still differ from the general population?

## Exercise 2

***Estimating crude death rates with a non-probability sample***

In Exercise 2, we are going to apply a new method for estimating death rates in humanitarian emergencies. This new method combines the *network survival method* (described below) with a non-probability quota sample—where researchers define a sample on a predetermined set of characteristics or quotas. This type of sample is one that researchers can realistically collect in a humanitarian emergency, such as a civil war. In such humanitarian emergencies, researchers often face significant barriers to collecting probability samples, mandating non-probability approaches to data collection.

The data collected for this exercise come from the Tanganyika Province of the Democratic Republic of the Congo (DRC). Supervised survey enumerators collected data from major transit and economic hubs, including taxi stations, ports, markets, and hospitals using a quota sample design. The quotas were established based on gender and geographic region.

In this exercise, we will assess the impact of two different weighting strategies, poststratification and inverse-probability weighting, on crude death rate estimates.

We want to construct weights as we suspect that the respondents in our non-probability sample may systematically differ from people in the general population we want to learn about.

To begin, we will read in the data we'll need for this analysis:

```{r}
## quota sample 
quota_sample <- read_csv("../../data/network_survival_quota_sample.csv") 

## read in poststratification weights (update path)
poststrat_weighting_targets <- read_csv("../../data/weighting_targets_poststrat.csv")

## probability sample 
probability_sample <- read_csv("../../data/network_survival_probability_sample.csv") 
```

Our survey asks respondents to report on two different social networks:

-   Closest neighbours (five closest neighbouring households by walking distance)

-   Kin (children, siblings, parents, grandparents, grandchildren, aunts/uncles)

For both groups, we have information for each survey respondent about (i) number of people in each group that they are connected to and (ii) number of people in each group that died in the past 120 days.

## Network survival

We will estimate a crude death rate, expressed as total deaths per 10,000 person days. This is the conventional way actors in humanitarian spaces express crude death rates; it contrasts with the conventional demographic definition of deaths per 1,000 person-years.

To produce an *estimate* of the crude death rate (our *estimand* of interest), we use the following *estimator*:

$$
\widehat{M} = \left( \frac{\sum_{i \in s} w_i~y_{i, D} } {\sum_{i \in s} w_i~d_i~E_{i} } \right) \times 10,000.
$$

where

-   $y_{i, D}$ is the total number of deaths reported by respondent $D$

-   $d_i$ is the degree (e.g., number of kin that respondent $i$ is connected to)

-   $E_i$ is the number of days of exposure that respondent $i$ reported (in this case, 120 days)

-   $\sum_{i \in s}$ is the sum over every person $i$ the sample $s$

-   $w_i$ is a survey weight

Let's describe this equation in words. To calculate a death rate, we require two pieces of information:

-   Number of deaths: this is simply the weighted total number of kin (neighbour) deaths reported by everyone in the sample.

<!-- -->

-   Measure of exposure: this is the sum of the weighted total number of kin (neighbours) reported by everyone in the sample, multiplied by their number of days of exposure they're reporting on (120 days, in this study).

The crude death rate is then obtained by dividing the total number of deaths by the total exposure and multiplying by 10,000 to express the rate in deaths per 10,000 person days.

We will compute this separately for the kin and neighbour ties, producing two distinct estimates of the crude death rate.

```{r}
# Estimate crude death rate (using neighbour ties)
quota_sample %>% 
  filter(!is.na(num_deaths_kin)) %>% 
  summarize(kin_death_rate_numerator = sum(num_deaths_kin), ## sum up all kin deaths 
            kin_death_rate_denominator = sum(num_total_kin) * 120) %>%  ## sum up all kin and multiply by 120 days of exposure 
  mutate(death_rate = 10000 * (kin_death_rate_numerator/kin_death_rate_denominator))

# Estimate crude death rate (using neighbour ties)
quota_sample %>% 
  filter(!is.na(num_deaths_neighbours)) %>% 
  summarize(neighbour_death_rate_numerator = sum(num_deaths_neighbours), ## sum up all kin deaths 
            neighbour_death_rate_denominator = sum(num_total_neighbour) * 120) %>%  ## sum up all kin and multiply by 120 days of exposure 
  mutate(death_rate = 10000 * (neighbour_death_rate_numerator/neighbour_death_rate_denominator))
```

As we're going to estimate the crude death rate many times in this analysis, we will write a dedicated function. The function has the following arguments:

-   `deaths`: the column for reported deaths for the social network (i.e., kin or neighbour deaths)

-   `degree`: the column for reported connections in the social network (i.e., network or neighbour degree)

-   `weights`: the column for weights

-   `exposure_days` the number of days of exposure reported by each person (i.e., how long the mortality report window is in days)

If no weights are supplied, this function will generate unweighted estimates.

```{r}
## network survival estimator function 
network_survival_estimator <- function(data, deaths, degree, weights = NULL, exposure_days = 120) {
  
  ## calculate cdr 
  data %>%
    summarise(
      death_rate_numerator = sum(if (!is.null(weights)) .data[[weights]] * .data[[deaths]] else .data[[deaths]], na.rm = TRUE),
      death_rate_denominator = sum(if (!is.null(weights)) .data[[weights]] * .data[[degree]] else .data[[degree]], na.rm = TRUE) * exposure_days
    ) %>%
    mutate(death_rate = 10000 * (death_rate_numerator / death_rate_denominator))
}

```

Let's try applying the function:

```{r}
## calculate kin crude death rate for kin
kin_estimates <- network_survival_estimator(data = quota_sample,
                                            deaths = "num_deaths_kin",
                                            degree = "num_total_kin",
                                            weights = NULL,
                                            exposure_days = 120) %>% 
  mutate(tie = "kin")

## calculate neighbour crude death rate for neighbours 
neighbour_estimates <- network_survival_estimator(data = quota_sample,
                                                 deaths = "num_deaths_neighbours",
                                                 degree = "num_total_neighbour",
                                                 weights = NULL,
                                                 exposure_days = 120) %>% 
  mutate(tie = "neighbour")

## combine crude death rate estimates together 
cdr_unweighted <- bind_rows(kin_estimates, neighbour_estimates) %>% 
  mutate(weights = "Unweighted")

## rpint out death rate estimates 
cdr_unweighted
```

#### Poststratification weights

First, we'll construct poststratification weights. We'll use the same general approach as we did in Exercise 1.

```{r}
## Look at weighting targets 
poststrat_weighting_targets
```

We'll construct the poststratification weights using cells defined by `age_class` and `gender`.

```{r}
# Poststratification population proporitons 
poststrat_population <- poststrat_weighting_targets %>%
  group_by(age_class, gender) %>%
  summarize(n = sum(population), .groups = "drop") %>%
  mutate(prop_pop = n / sum(n)) %>%
  ungroup()

# Poststratification sample proportions 
poststrat_sample <- quota_sample %>%
  group_by(age_class, gender) %>%
  summarize(n = n(), .groups = "drop") %>%
  mutate(prop_sample = n / sum(n)) %>%
  ungroup()

# Calculate poststratification weights
poststrat_weights <- poststrat_sample %>%
  inner_join(poststrat_population, by = c("age_class", "gender")) %>%
  mutate(weight = prop_pop / prop_sample) %>%
  dplyr::select(gender, age_class, weight_poststrat = weight)

# Joining the poststratification weights back into survey_df
quota_sample <- quota_sample %>%
  left_join(poststrat_weights, by = c("gender", "age_class")) 

# Standardize weights so mean is 1 
quota_sample <- quota_sample %>%
  mutate(weight_poststrat = weight_poststrat / mean(weight_poststrat)) 

```

Now we can visualize the distribution of weights.

```{r}
## quota sample 
quota_sample %>% 
  ggplot() + 
  geom_histogram(aes(x = weight_poststrat)) + 
  theme_cowplot()
```

The distribution of weights looks reasonable. As a general rule of thumb, we do not want any weights to be more than 5 times larger than our average weight. Here, we do not observe any extreme weights. If there are extreme weights, a common approach is to "trim" the weights — for instance, any weight greater than 5 is assigned a value of 5.

Now we're read to calculate the weighted crude death rate estimates. Note that we need to explicitly tell our function what weight column to use - in this case, `weight_poststrat`.

```{r}
## crude death rate estimates (kin) - with poststratification weights 
kin_estimates_poststrat <- network_survival_estimator(data = quota_sample,
                                                      deaths = "num_deaths_kin",
                                                      degree = "num_total_kin",
                                                      weights = "weight_poststrat",
                                                      exposure_days = 120) %>% 
  mutate(tie = "kin")

## crude death rate estimates (neighbour) - with poststratification weights 
neighbour_estimates_poststrat <- network_survival_estimator(data = quota_sample,
                                                           deaths = "num_deaths_neighbours",
                                                           degree = "num_total_neighbour",
                                                           weights = "weight_poststrat",
                                                           exposure_days = 120) %>% 
  mutate(tie = "neighbour")

## Combine estimates 
cdr_poststratification <- bind_rows(kin_estimates_poststrat, neighbour_estimates_poststrat) %>% 
  mutate(weights = "Poststratification Weights")

## Print out poststratification estimates 
cdr_poststratification
```

We can now check whether the new poststratification weighted estimates are higher than the weighted estimates:

```{r}
## combine crude death rate estimates with unweighted estimates 
cdr_estimates <- cdr_poststratification  %>% 
  bind_rows(cdr_unweighted)

## Visualize crude death rate estimates 
cdr_estimates %>% 
  ggplot(aes(y = death_rate, x = tie, fill = weights)) + 
  geom_col(size = 3, position = position_dodge2(.3)) + 
  theme_cowplot() + 
  ylim(0, .45) + 
  theme(legend.position = "bottom") + 
  labs(x = "",
       y = "Crude Death Rate \n (per 10,000 person days)") + 
  geom_text(aes(y = death_rate + .02, label = round(death_rate, 2)), 
            position = position_dodge2(1))
```

Our poststratification weights increased our estimates of the crude death rate, but only by about 10%.

## Inverse probability weights (IPW)

Our poststratification weights only account for age class and gender. But there might be important selection along other dimensions. For example, we may suspect that wealthier individuals are more likely to have social networks with lower mortality.

To better adjust for this, we need our weighting strategy to account for additional dimensions of selection into our sample. However, the challenge with poststratification weighting is the "curse of dimensionality": as the number of stratification variables increases, cells become sparse and can produce extremely large and unstable weights.

Instead of poststratification, we will use inverse-probability weighting (IPW) to estimate each respondent's probability of inclusion in the non-probability sample. Here, we have collected an auxiliary probability-based sample to help with reweighting. The inverse probability weighting will proceed in three steps:

1.  Append together our non-probability (quota) sample and our auxiliary probability sample
2.  Fit a logistic regression predicting inclusion probability in the non-probability sample
3.  Calculate weights as inverse of inclusion probability

```{r}
## read in auxiliary probability sample 
probability_sample <- probability_sample %>%
  mutate(inclusion = 0)

## create inclusion variable in 
quota_sample <- quota_sample %>% 
  mutate(inclusion = 1)

## append the probability and non-probability samples together 
pooled_sample <- bind_rows(probability_sample, quota_sample)

## fit a logistic regression
selection_model <- glm(inclusion ~ gender + age_class, data = pooled_sample, family = "binomial")

## print summary of model 
summary(selection_model)

## generate ipw weights 
inclusion_prob <- predict(object = selection_model, newdata = quota_sample, type = "response")

## quota sample 
quota_sample <- quota_sample %>% 
  mutate(weight_ipw = 1/inclusion_prob) %>% 
  mutate(weight_ipw = weight_ipw/ mean(weight_ipw)) ## re-scale weights 
```

Now let's create a histogram of our inverse probability weights (IPW).

```{r}
## histogram of weights 
quota_sample %>% 
  ggplot() +  
  geom_histogram(aes(x = weight_ipw)) +
  theme_cowplot()
```

The distribution of the weights looks reasonable; there are no extreme weights. We can now calculate our crude death rate using our new inverse probability weights.

```{r}
## calculate cdr with inverse probability weights (kin)
kin_estimates_ipw <- network_survival_estimator(data = quota_sample,
                                                      deaths = "num_deaths_kin",
                                                      degree = "num_total_kin",
                                                      weights = "weight_ipw",
                                                      exposure_days = 120) %>% 
  mutate(tie = "kin")

## calculate cdr with inverse probability weights (neighbour)
neighbour_estimates_ipw <- network_survival_estimator(data = quota_sample,
                                                           deaths = "num_deaths_neighbours",
                                                           degree = "num_total_neighbour",
                                                           weights = "weight_ipw",
                                                           exposure_days = 120) %>% 
  mutate(tie = "neighbour")

## combine together estimates 
cdr_ipw <- bind_rows(kin_estimates_ipw, neighbour_estimates_ipw) %>% 
  mutate(weights = "Inverse Probability Weights")
```

Now we can compare our estimates of the crude death rate using (i) no weights, (ii) poststratification weights, and (iii) inverse probability weights.

```{r}
## inverse probability weights 
cdr_estimates %>% 
  bind_rows(cdr_ipw) %>% 
    mutate(weights = factor(weights, levels = c("Unweighted", "Poststratification Weights", "Inverse Probability Weights"))) %>%  # Reorder weights
  ggplot(aes(y = death_rate, x = reorder(tie, -death_rate), fill = weights)) + 
  geom_col(size = 3, position = position_dodge2(.3)) + 
  theme_cowplot() + 
  ylim(0, .5) + 
  theme(legend.position = "bottom") + 
  labs(x = "",
       y = "Death Rate") + 
  geom_text(aes(y = death_rate + .02, label = round(death_rate, 3)), 
            position = position_dodge2(1))

```

Our estimates are very similar, regardless of whether we use poststratification weights or inverse probability weights. However, we have more covariates in our non-probability that may better capture socioeconomic status (SES), which our theoretical intuition tells us may be important...

### Exercise 2 questions:

We're worried that our quota sample is over-representing higher SES individuals. To address this, we'll incorporate additional covariates into our logistic regression model which is estimating each respondent's inclusion probability.

**2.1** Recalculate the inverse probability weights using a new logistic regression model with the following additional predictors: `manufactured_material_house,` `bed,` `radio,` and `modern_fuel_type`. These covariates provide a proxy measure of wealth, measuring whether: (i) a respondent's home was built with modern construction materials (1 = yes, 0 = no), (ii) respondent owns a bed, (iii) respondent owns a radio, and (iv) respondent uses modern fuel for cooking. Are these predictors statistically significant in our new logistic regression model?

**2.2** Re-estimate the crude death rate using the new weights (using both kin and neighbour networks). How does your crude death rate change? Make a plot comparing the unweighted estimate, poststratification estimate, and the new inverse probability weighted estimate.

**2.3** Speculate on why our estimated crude death rates changed under inverse probability weighting. What does this tell us about the mortality in the kin and neighbor networks of low vs. high SES respondents?

## Bonus exercise

Calculate separate crude death rates for each health zone. Use no weights, poststratification weights, and inverse probability weights (including SES variables). Which health zone has the highest crude death rate?
